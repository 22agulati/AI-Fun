{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Arnav's CS188 WHW4 Notebook.ipynb","provenance":[{"file_id":"1XdKKwAW-fOk5MGyMu4vKWPSOc-H837eD","timestamp":1606294602213},{"file_id":"1XKvj8uPDGk_YJXba4Fz2Lu2KmSGR2qBk","timestamp":1587622708067},{"file_id":"115TIo5OkjelMY0NOYnIpQAheyGNpvCMQ","timestamp":1587096364340},{"file_id":"1lIpUJW3EN_klPfUWDN-rNK-aCGFaPjzu","timestamp":1587070971152}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"442ec9175a0e4175a8058d6f280acb6d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_f5cc0b4f8521466e8ac84deea4d894ef","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9fe29786139d44f6b41dab61d2952b54","IPY_MODEL_a0f556b355cd4756878d26b47f56f974"]}},"f5cc0b4f8521466e8ac84deea4d894ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9fe29786139d44f6b41dab61d2952b54":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1304c4091ea04ebfb23658614c00280f","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":9143470,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":9143470,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_66235bc593c94ba6b5076127b871ca6a"}},"a0f556b355cd4756878d26b47f56f974":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9219309d063940f2a08b91162fefc74f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 9.14M/9.14M [00:36&lt;00:00, 253kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c4d3653426ba43038d74ae04ff0f4555"}},"1304c4091ea04ebfb23658614c00280f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"66235bc593c94ba6b5076127b871ca6a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9219309d063940f2a08b91162fefc74f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c4d3653426ba43038d74ae04ff0f4555":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3c705fe2aa0f4edcbd2e768b5b090693":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_123d63bf5c6148be8ec64a2997ac98ba","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_bc9f504dc3a345bfa0c9e4fae66b296f","IPY_MODEL_79bc2b1632054ee2a6a3db6991e87682"]}},"123d63bf5c6148be8ec64a2997ac98ba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bc9f504dc3a345bfa0c9e4fae66b296f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e0484983966c462f9da0bd18cbe3ae9b","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":856,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":856,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_125999ddb9314f71b389d40c253a3879"}},"79bc2b1632054ee2a6a3db6991e87682":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8587e8826cfd491ca94e7a2b386b79b4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 856/856 [00:00&lt;00:00, 939B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_209f8d6905514667b652b1d589508062"}},"e0484983966c462f9da0bd18cbe3ae9b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"125999ddb9314f71b389d40c253a3879":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8587e8826cfd491ca94e7a2b386b79b4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"209f8d6905514667b652b1d589508062":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a610e1c0f7004f729bb40cc89eab8dac":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7549d3d806794503b1d5b03d28dced3f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f31341c6e23b4eeba46fac3d5cbaff31","IPY_MODEL_41e7d666b8b14aa4a94a9ae819721872"]}},"7549d3d806794503b1d5b03d28dced3f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f31341c6e23b4eeba46fac3d5cbaff31":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_fee6fd2a77f149f38c471d19a5767f70","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1140884800,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1140884800,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7fc1ca892f9949df9a215d17578e2dae"}},"41e7d666b8b14aa4a94a9ae819721872":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ead8b69e75a54a26ae85c0d84f5f81e9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.14G/1.14G [00:33&lt;00:00, 34.4MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2aa3650449d94334a8c1708a5a6ea31e"}},"fee6fd2a77f149f38c471d19a5767f70":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"7fc1ca892f9949df9a215d17578e2dae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ead8b69e75a54a26ae85c0d84f5f81e9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2aa3650449d94334a8c1708a5a6ea31e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"r_Xl4tym_lSS"},"source":["# **Written HW 4: Code Portion**\n","\n","# There are two things you need to do before anything else:\n","\n","1. File -> Save a Copy in Drive\n","2. In your own copy of the colab notebook, make sure you use a **GPU**! Go to Runtime -> Change runtime type -> set Hardware accelator to GPU. \n","\n","<!--<img src=\"https://upload.wikimedia.org/wikipedia/commons/b/bb/Defense.gov_photo_essay_100124-D-0000G-001.jpg\" alt=\"A U.S. Navy sailor carries a Haitian boy off a helicopter at Terminal Varreux, Haiti, Jan. 23, 2010, as the boy's mother follows behind them. The child received treatment aboard one of the U.S. Navy ships serving as a hospital in Port-au-Prince harbor, Haiti and he was later discharged. U.S. military personnel are providing aid and support to earthquake victims in Haiti.\" width=\"400\"/>-->\n","\n","\n","### Colaboratory\n","\n","You are currently using a Colaboratory notebook, which runs Jupyter notebooks. There are text cells and code cells, and you'll be writing mostly in code cells, though feel free to double click and edit text cells to write answers to the questions we've sprinkled throughout (marked **Q**). \n","\n","A key idea is to make sure you run cells in order, and double check that you've run previous cells before running a current cell. When you declare a variable or function in a cell, you need to run it so the notebook \"remembers\" it for use in future cells. \n","\n","To run a cell, press Shift->Enter or click on the play button in the top left corner of the cell. \n","\n","Feel free to go to Tools -> Preferences -> Miscellaneous to adjust some fun settings. "]},{"cell_type":"markdown","metadata":{"id":"j9cLeLUB3go4"},"source":["# Q1: Probabilistic Language Modeling: Coding Portion\n","\n","\n","These are the functions you need to fill in in the N-gram class. Please only add code between the \"\"\" START YOUR CODE HERE \"\"\" and \"\"\" END YOUR CODE HERE \"\"\" comments and don't change anything else or else Staff will have a really hard time helping you debug in OH:\n","1. count_words\n","2. calc_word_probs\n","3. probs_to_neg_log_probs\n","4. filter_adj_counter\n","5. p_naive\n","6. calc_neg_log_prob_of_sentence\n","7. calc_prob_of_sentence"]},{"cell_type":"code","metadata":{"id":"J461sDp1mTdJ","executionInfo":{"status":"ok","timestamp":1606324670805,"user_tz":-330,"elapsed":1905,"user":{"displayName":"Arnav Gulati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXwjHg14lrUBqdz4gJ-8OGJt5S2ZDu5Pp8bMQr=s64","userId":"07163756331818374591"}}},"source":["import nltk\n","from nltk.tokenize import RegexpTokenizer\n","import collections\n","import string\n","import re\n","import numpy as np\n","import random\n","import matplotlib.pyplot as plt\n","import time\n","import itertools"],"execution_count":48,"outputs":[]},{"cell_type":"code","metadata":{"id":"CH9kFPnZFbOS","executionInfo":{"status":"ok","timestamp":1606324671747,"user_tz":-330,"elapsed":2256,"user":{"displayName":"Arnav Gulati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXwjHg14lrUBqdz4gJ-8OGJt5S2ZDu5Pp8bMQr=s64","userId":"07163756331818374591"}}},"source":["class N_gram:\n","    def __init__(self, corpora_path_list, N, regexp_tokenizer=True, add_periods=False, case_sensitive=False):\n","        self.N = N\n","        self.corpora_path_list = corpora_path_list\n","        self.text_list = None\n","        self.adj_counters = None\n","        self.phrases_sets = None\n","        self.V = None\n","        self.regexp_tokenizer = regexp_tokenizer\n","        self.add_periods = add_periods\n","        self.case_sensitive = case_sensitive\n","\n","    def train(self):\n","        print(\"Preprocessing training corpora...\")\n","        if self.text_list is None:\n","            self.text_list = self.preprocess_corpus(self.corpora_path_list)\n","            print(\"Training set size:\", len(self.text_list), \"words\")\n","        if self.case_sensitive:\n","            print(\"Recording all variations of capitalization...\")\n","            self.capsVariations = self.recordCapsVariations()\n","        print(\"Constructing Phrase/Adjacency counters...\")\n","        self.adj_counters = self.calc_adj_counters(self.N)\n","        print(\"Constructing List of Phrase Sets...\")\n","        self.phrases_sets = [set(self.list_phrases(i)) for i in range(self.N + 1)]\n","        self.V = len(set(self.text_list)) # size of vocab (# unique words in corpora)\n","        print(\"Done Training.\")\n","\n","    def preprocess_corpus(self, text_file_path_list):\n","        \"\"\"\n","        Input: String of text_file_path.\n","        Returns: List format of text, as divided by regex delimiters.\n","        Either tokenizes punctuation or not, depending on RegexpTokenizer parameter;\n","        Lowercases all words.\n","        \n","        >>> c = N_gram([**comma-separated corpora filepaths**], 2)\n","        >>> c.preprocess_corpus([\"carter_concession.txt\"])\n","        [\"i\", \"promised\", \"you\", \"four\", \"years\", \"ago\", ...]\n","        \"\"\"\n","        assert isinstance(text_file_path_list, list), \"text_file_path_list not a list.\"\n","        def txt_to_str(text_file_path_list):\n","            aggregated_str = \"\"\n","            for text_file_path in text_file_path_list:\n","                f = open(text_file_path)\n","                aggregated_str += f.read()\n","            return aggregated_str\n","        \n","        raw_text = txt_to_str(text_file_path_list)\n","        if self.add_periods:\n","            sentences_list = raw_text.split(\"\\n\")\n","            text_list = [\".\"] + [token \n","                for sentence in [sentence.split() + [\".\"] for sentence in sentences_list] \n","                for token in sentence]\n","        else:\n","            tokenizer = nltk.tokenize.RegexpTokenizer('[\\w\\']+|[.$%!?]+')\n","            if not self.regexp_tokenizer: # Use whitespace tokenizer\n","                tokenizer = nltk.tokenize.regexp.WhitespaceTokenizer()\n","            #Include words/numbers as one token, periods/$/% as another.\n","            #[\\w\\'] allows us to ensure that contractions like I'm are preserved as one token.\n","            text_list = tokenizer.tokenize(raw_text)\n","            if not self.case_sensitive:\n","                text_list = [\".\"] + [word.lower() for word in text_list] # lowercase everything\n","        return text_list\n","\n","    def list_phrases(self, n):\n","        \"\"\"\n","        Creates a list of tuples of adjacent word pairs.\n","        \n","        >>> c = N_gram([**comma-separated corpora filepaths**], 2)\n","        >>> c.text_list = [\"This\", \"is\", \"Sam\", \".\"]\n","        >>> c.list_phrases(2)\n","        [(\"This\", \"is\"), (\"is\", \"Sam\"), (\"Sam\", \".\")]\n","        \"\"\"\n","        return [tuple([self.text_list[i + j] for j in range(n)]) \n","            for i in range(0, len(self.text_list) - n + 1)]\n","\n","    def normalize_dict(self, d):\n","        \"\"\"\n","        Creates copy of d;\n","        Sums all keys of the dictionary d and divides each key by the total;\n","        returns the new dict.\n","\n","        >>> c = N_gram([**comma-separated corpora filepaths**], 2)\n","        >>> d = {1 : 2, 2 : 3}\n","        >>> c.normalize_dict(d)\n","        {1 : 0.4, 2 : 0.6}\n","        \"\"\" \n","        normalized_dict = dict(d)\n","        dict_vals_total = sum(d.values())\n","        normalized_dict.update((key, val / dict_vals_total) for key, val in normalized_dict.items())\n","        return normalized_dict\n","\n","    def sample_dict(self, d):\n","        \"\"\"\n","        Sample a key from a dictionary of counts.\n","        \"\"\"\n","        assert d, \"dictionary is empty\"\n","        d = self.normalize_dict(d)\n","        # print(d)\n","        rand = random.random()\n","\n","        cumProb = 0.0\n","        for currKey in d:\n","            cumProb += d[currKey]\n","            if rand <= cumProb:\n","                return currKey\n","        return currKey\n","\n","    def count_words(self):\n","        \"\"\"\n","        Returns a dictionary of the words as keys and their counts as values.\n","\n","        >>> c = N_gram([**comma-separated corpora filepaths**], 2)\n","        >>> c.text_list = [\"i\", \"need\", \"i\", \"have\"]\n","        >>> c.count_words()\n","        {\"i\": 2, \"need\": 1, \"have\": 1}\n","        \"\"\"\n","        ### START YOUR CODE HERE ###\n","        mapping = {}\n","        for word in self.text_list:\n","          mapping[word] = mapping.get(word, 0) + 1\n","        return mapping\n","        ### END YOUR CODE HERE ###\n","\n","    def calc_word_probs(self):\n","        \"\"\"\n","        Returns a dictionary with the sample probability of drawing each word.\n","\n","        >>> c = N_gram([**comma-separated corpora filepaths**], 2)\n","        >>> c.text_list = [\"i\", \"need\", \"i\", \"have\"]\n","        >>> c.calc_word_probs()\n","        {\"i\": 0.5, \"need\": 0.25, \"have\": 0.25}\n","        \"\"\"\n","        ### START YOUR CODE HERE ###\n","        counts = self.count_words()\n","        return self.normalize_dict(counts)\n","        ### END YOUR CODE HERE ###\n","\n","    def probs_to_neg_log_probs(self, probs_dict):\n","        \"\"\"\n","        Convert dictionary of probabilities into dictionary of negative ln probabilities.\n","        \"\"\"\n","        ### START YOUR CODE HERE ###\n","        neg_log_probs = {}\n","        for key in probs_dict.keys():\n","          neg_log_probs[key] = -np.log(probs_dict[key])\n","        ### END YOUR CODE HERE ###\n","        return neg_log_probs\n","\n","    def calc_neg_log_word_probs(self):\n","        \"\"\"\n","        Convert text list into dictionary of negative ln probabilities.\n","\n","        >>> c = N_gram([**comma-separated corpora filepaths**], 2)\n","        >>> c.text_list = [\"i\", \"need\", \"i\", \"have\"]\n","        >>> c.calc_neg_log_word_probs()\n","        {\"i\": -0.69, \"need\": -1.39, \"have\": -1.39}\n","        \"\"\"\n","        word_probs = calc_word_probs()\n","        return self.probs_to_neg_log_probs(word_probs)\n","\n","    def calc_adj_counter(self, n):\n","        \"\"\"\n","        Convert text list into dictionary of counts of phrases of length n.\n","\n","        >>> c = N_gram([], 2)\n","        >>> c.text_list = [\"i\", \"have\", \"a\", \"dream\", \".\", \"i\", \"have\"]\n","        >>> c.calc_adj_counter(3)\n","        {(\"i\", \"have\"): 2, (\"have\", \"a\"): 1, (\"a\", \"dream\"): 1, (\"dream\", \".\"): 1, (\".\", \"i\"): 1}\n","        \"\"\"\n","        return collections.Counter(self.list_phrases(n))\n","\n","    def calc_adj_counters(self, n):\n","        \"\"\"\n","        Create a list of adj_counters of phrase length 1,...,n on text_list.\n","\n","        >>> c = N_gram([**comma-separated corpora filepaths**], 2)\n","        >>> c.text_list = [\"eye\", \"for\", \"eye\"]\n","        >>> c.calc_adj_counters(3)\n","        [0,\n","        {(\"eye\",): 2, (\"for\"): 1},\n","        {(\"eye\", \"for\"): 1, (\"for\", \"eye\"): 1}\n","        {(\"eye\", \"for\", \"eye\"): 1}]\n","        \"\"\"\n","        adj_counters = [0 for i in range(n + 1)]\n","        for i in range(1, n + 1):\n","            adj_counters[i] = self.calc_adj_counter(i)\n","        return adj_counters\n","\n","    def calc_adj_probs(self, adj_counter):\n","        \"\"\"\n","        Convert adj_counter values from counts to sample probabilities.\n","\n","        >>> c = N_gram([**comma-separated corpora filepaths**], 2)\n","        >>> adj_counter = {(\"hey\", \"you\"): 2. (\"you\", \".\"): 3}\n","        >>> c.calc_adj_probs(adj_counter)\n","        {(\"hey\", \"you\"): 0.4. (\"you\", \".\"): 0.6}\n","        \"\"\"\n","        return self.normalize_dict(adj_counter)\n","\n","    def filter_adj_counter(self, adj_counter, word_tuple, n):\n","        \"\"\"\n","        Returns: Dictionary with num_occurrences of word pairs starting with 'word_tuple'.\n","\n","        >>> c = N_gram([**comma-separated corpora filepaths**], 2)\n","        >>> word_tuple = (\"Jimmy\",)\n","        >>> adj_counter = {(\"never\", \"lie\"): 3, (\"hey\", \"Jimmy\"): 2, \n","            (\"Jimmy\", \"Li\"): 1, (\"Jimmy\", \"Carter\"): 100}\n","        >>> c.filter_adj_counter(adj_counter, word_tuple, 2)\n","        {(\"Jimmy\", \"Li\"): 1, (\"Jimmy\", \"Carter\"): 100}\n","        \"\"\"\n","        assert len(word_tuple) in range(n + 1), \\\n","            \"word_tuple does not have valid length: \" + str(len(word_tuple))\n","\n","\n","        if not adj_counter:\n","            adj_counter = self.calc_adj_counter(n)\n","        subset_word_adj_counter = {}\n","        \n","        if not word_tuple:\n","            return dict(self.calc_adj_counter(1))\n","        elif len(word_tuple) == n:\n","            return {word_tuple: adj_counter[word_tuple]}\n","\n","        for phrase in adj_counter.keys():\n","            ### START YOUR CODE HERE ###\n","            extracted = phrase[:len(word_tuple)]\n","            if extracted == word_tuple:\n","              subset_word_adj_counter[phrase] = adj_counter[phrase]\n","            ### END YOUR CODE HERE ###\n","        return subset_word_adj_counter\n","\n","    def perplexity(self, sentence_list, n):\n","        \"\"\"\n","        Returns perplexity of sentence_list given:\n","        --text_list: list of tokens to train on.\n","        --adj_counters: list of all 1,...,n adj_counter. Uses the output of calc_adj_counters(.,.)\n","        --n: The N in N-gram.\n","\n","        >>> c = N_gram([**comma-separated corpora filepaths**], 2)\n","        >>> c.train()\n","        >>> sentence_list = [\"super\", \"confusing\", \"sentence\"]\n","        >>> c.perplexity(sentence_list, 2)\n","        2000.01\n","        \"\"\"\n","        text_neg_log_prob = self.calc_neg_log_prob_of_sentence(sentence_list, n, self.p_KN)\n","        return np.exp(text_neg_log_prob / len(sentence_list))\n","\n","    def p_naive(self, curr_word, prev_phrase, n):\n","        \"\"\"\n","        Calculates the sample probability of all length-`n` phrases in `text_list`\n","\n","        >>> c = N_gram([**comma-separated corpora filepaths**], 2)\n","        >>> c.text_list = [\".\", \"i\", \"think\", \".\", \"i\", \"am\", \".\", \"why\", \"yes\", \".\"]\n","        >>> c.p_naive(\"i\", (\".\",), 2)\n","        0.666 # Because 3 length-2 phrases start with (\".\",), and of the three, 2 of them end with \"curr_word\"\n","        \"\"\"\n","        assert n >= 1, \"Invalid value of n.\"\n","        assert len(prev_phrase) < n, \"Length of prev_phrase not < n.\"\n","        assert isinstance(prev_phrase, tuple), \"prev_phrase is not a tuple: \" + str(prev_phrase)\n","        filtered_adj_counter = self.filter_adj_counter(None, prev_phrase, n)\n","        try:\n","            ### START YOUR CODE HERE ###\n","            filtered_adj_counter = self.normalize_dict(filtered_adj_counter)\n","            prob = 0\n","            for phrase in filtered_adj_counter.keys():\n","              extracted = phrase[n - 1]\n","              if extracted == curr_word:\n","                prob += filtered_adj_counter[phrase]\n","            ### END YOUR CODE HERE ###\n","        except KeyError:\n","            print(prev_phrase + (curr_word,), \" has probability 0.\")\n","            prob = 0\n","        return prob\n","\n","    def p_laplace(self, curr_word, prev_phrase, n, k=0.1):\n","        \"\"\"\n","        Calculates the sample probability of all length-`n` phrases in `text_list`\n","        And performs k-smoothing.\n","        \"\"\"\n","        assert n >= 1, \"Invalid value of n.\"\n","        assert len(prev_phrase) < n, \"Length of prev_phrase not < n.\"\n","        assert isinstance(prev_phrase, tuple), \"prev_phrase is not a tuple: \" + str(prev_phrase)\n","        \n","        filtered_adj_counter = self.filter_adj_counter(None, prev_phrase, n)\n","        num_entries = len(filtered_adj_counter.keys())\n","        laplace_denominator = sum(filtered_adj_counter.values()) + k * (num_entries + 1)\n","        smoothed_prob_dict = dict(filtered_adj_counter)\n","        smoothed_prob_dict.update((key, (val + k) / laplace_denominator) for key, val in smoothed_prob_dict.items())\n","\n","        unk_prob = k / laplace_denominator # out of distribution words\n","\n","        err_tolerance = 1e-4\n","        err_msg = \"Smoothed probabilities don't add up to 1!\"\n","\n","        assert np.abs(sum(smoothed_prob_dict.values()) + unk_prob - 1) < err_tolerance, err_msg\n","\n","        try:\n","            prob = smoothed_prob_dict[prev_phrase + (curr_word,)]\n","        except KeyError:\n","            prob = unk_prob\n","            # print(prev_phrase + (curr_word,), \" has probability 0. Replaced with prob\", prob)\n","        # print(\"smoothed_prob_dict\", smoothed_prob_dict)\n","        # print(\"prev_phrase\", prev_phrase)\n","        # print(\"curr_word\", curr_word)\n","        return prob\n","\n","    def calc_neg_log_prob_of_sentence(self, sentence_list, n, p_func=p_laplace):\n","        \"\"\"\n","        Return negative log probability of `sentence_list` occurring, given:\n","        --n: The n in n-gram.\n","        --p_func: must contain args (curr_word, prev_phrase, text_list, adj_counters, n) in order.\n","        \"\"\"\n","        assert len(sentence_list) > 0, \"Empty sentence.\"\n","        adj_probs = self.calc_adj_probs(self.adj_counters[n])\n","        assert len(list(adj_probs.keys())[0]) == n, (\n","            \"Non-matching dimension of adj_probs keys and n.\")\n","        cum_neg_log_prob = 0\n","        if sentence_list[0] != \".\":\n","            sentence_tuple = (\".\",) + tuple(sentence_list)\n","        else:\n","            sentence_tuple = tuple(sentence_list)\n","\n","        for i in range(0, len(sentence_tuple) - 1):\n","            prev_phrase = sentence_tuple[max(0, i - n + 2): i + 1]\n","            assert len(prev_phrase) < n, (\"Invalid length of prev_phrase:\" \n","                + str(len(prev_phrase)) + \", n: \" + str(n))\n","            curr_word = sentence_tuple[i + 1]\n","\n","            # The following evaluates to P(currToken | previous N - 1 tokens)\n","            curr_word_prob = p_func(self, \n","                curr_word, \n","                prev_phrase, \n","                min(len(prev_phrase) + 1, n)\n","            ) #defaults to using p_naive\n","\n","            ### START YOUR CODE HERE ###\n","            cum_neg_log_prob += -np.log(curr_word_prob)\n","            ### END YOUR CODE HERE ###\n","        return cum_neg_log_prob\n","\n","    def calc_prob_of_sentence(self, sentence_list, n, p_func=p_laplace):\n","        \"\"\"\n","        Convert a sentence's neg_log_prob to prob.\n","        \"\"\"\n","        ### START YOUR CODE HERE ###\n","        neg_log_prob = self.calc_neg_log_prob_of_sentence(sentence_list, n, p_func)\n","        return np.exp(-neg_log_prob)\n","        ### END YOUR CODE HERE ###\n","\n","    def top_k_adj_starting_with(self, adj_counter, phrase, n, k):\n","        dict_counter = collections.Counter(self.filter_adj_counter(adj_counter, phrase, n))\n","        return dict(dict_counter.most_common(k))\n","\n","    def likeliest_adj_starting_with(self, adj_counter, phrase, n):\n","        \"\"\"\n","        Returns: Tuple with highest num_occurrences starting with `phrase`.\n","        >>> c = N_gram([**comma-separated corpora filepaths**], 2)\n","        >>> filtered_adj_counter = {(\"Jimmy\", \"Li\", \"is\"): 1, (\"Jimmy\", \"Li\", \"likes\"): 100}\n","        >>> phrase = (\"Jimmy\", \"Li\")\n","        >>> c.likeliest_adj_starting_with(filtered_adj_counter, phrase, 3)\n","        (\"Jimmy\", \"Li\", \"likes\") # most likely length-3 phrase starting with (\"Jimmy\", \"Li\")\n","        \"\"\" \n","        assert isinstance(phrase, tuple), \"Phrase is not a tuple.\"\n","        subset_word_adj_counter = self.filter_adj_counter(adj_counter, phrase, n)\n","        return max(subset_word_adj_counter, key=lambda adj: subset_word_adj_counter[adj])\n","\n","    def generate_sentence(self, k=5, T=1):\n","        \"\"\"\n","        Generate a sentence by sampling amongst the top k candidates when generating each token\n","        \"\"\"\n","        assert k >= 1, \"k is not at least 1\"\n","\n","        sentence = \"\"\n","        prev_phrase = (\".\",)\n","        curr_word = \".\"\n","        num_words_in_prev_phrase = 1\n","        while sentence == \"\" or curr_word != \".\":\n","            n_iter = min(num_words_in_prev_phrase + 1, self.N)\n","\n","            top_k_adj_counts = self.top_k_adj_starting_with(self.adj_counters[n_iter], prev_phrase, n_iter, k)\n","            # print(\"top_k_adj_counts\")\n","\n","            sampled_adj = self.sample_dict(top_k_adj_counts)\n","            \n","            prev_phrase, curr_word = sampled_adj[:-1], sampled_adj[-1]\n","            # sentence += (\" \" if curr_word != \".\" and sentence else \"\") + curr_word\n","            sentence += \" \" + curr_word\n","            if num_words_in_prev_phrase < self.N - 1:\n","                prev_phrase = prev_phrase + (curr_word,)\n","            else:\n","                prev_phrase = prev_phrase[1:] + (curr_word,)\n","\n","            num_words_in_prev_phrase = min(num_words_in_prev_phrase + 1, self.N)\n","\n","        return sentence\n","\n","    def generate_likeliest_sentence(self):\n","        \"\"\"\n","        Naive way of generating likeliest sentence. \n","        First chooses the likeliest word following a period (via `p_naive` technique)\n","        Then chooses the likeliest word following that word.\n","        Rinse and Repeat.\n","        Stop when likeliest word is \".\".\n","\n","        >>> c = N_gram([**comma-separated corpora filepaths**], 2)\n","        >>> c.generate_likeliest_sentence()\n","        \"we have been a year of the people .\"\n","        \"\"\"\n","        return self.generate_sentence(1)"],"execution_count":49,"outputs":[]},{"cell_type":"code","metadata":{"id":"T1C0ky7FtV1z","executionInfo":{"status":"ok","timestamp":1606324672324,"user_tz":-330,"elapsed":1483,"user":{"displayName":"Arnav Gulati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXwjHg14lrUBqdz4gJ-8OGJt5S2ZDu5Pp8bMQr=s64","userId":"07163756331818374591"}}},"source":["class CaseSensitive_N_gram(N_gram):\n","    def __init__(self, corpora_path_list, N, regexp_tokenizer=True, add_periods=False, case_sensitive=True):\n","        super().__init__(corpora_path_list, N, regexp_tokenizer, add_periods, case_sensitive)\n","        self.capsVariations = {}\n","\n","    def recordCapsVariations(self):\n","        \"\"\"\n","        Record all the capitalization variations of a word in self.capsVariations\n","        \"\"\"\n","        capsVariations = {}\n","\n","        for token in self.text_list:\n","            lowercase_token = token.lower()\n","            if lowercase_token not in capsVariations:\n","                capsVariations[lowercase_token] = [token] # create a list\n","            else:\n","                if token not in capsVariations[lowercase_token]:\n","                    capsVariations[lowercase_token].append(token)\n","\n","        return capsVariations\n","\n","    def caps_filtered_adj_counter(self, adj_counter, word_tuple, n):\n","        assert len(word_tuple) == n, \"len(word_tuple): {} != n: {}\".format(len(word_tuple), n)\n","        prec_phrase, last_word = word_tuple[:-1], word_tuple[-1]\n","        \n","        last_word_lowercase = last_word.lower()\n","\n","        case_adj_counter = {}\n","\n","        print(\"self.capsVariations[last_word_lowercase]\", self.capsVariations[last_word_lowercase])\n","        \n","        for last_word_caps_variation in self.capsVariations[last_word_lowercase]:\n","            word_tuple_variation = prec_phrase + (last_word_caps_variation,)\n","            case_adj_counter[word_tuple_variation] = adj_counter[word_tuple_variation]\n","        return case_adj_counter\n","\n","    def enumerate_caps_variation_nl_probs(self, adj_counter, sentence_list, n):\n","        lowercase_sentence_list = [token.lower() for token in sentence_list]\n","        word_list_caps_variations = []\n","\n","        for token in lowercase_sentence_list:\n","            word_list_caps_variations.append(self.capsVariations[token])\n","\n","        sentence_caps_variations = list(itertools.product(*word_list_caps_variations))\n","\n","        # print (\"sentence_caps_variations\", sentence_caps_variations)\n","        sentence_caps_variations_nl_probs = {}\n","\n","        for i, sentence_caps_variation in enumerate(sentence_caps_variations):\n","            if i % 10 == 0:\n","                print(\"[{}/{}]\".format(i, len(sentence_caps_variations)))\n","            variation_nl_prob = self.calc_neg_log_prob_of_sentence(list(sentence_caps_variation), n)\n","            sentence_caps_variations_nl_probs[sentence_caps_variation] = variation_nl_prob\n","\n","        return sentence_caps_variations_nl_probs\n","\n","    def truecase_sentence(self, adj_counter, sentence_list, n):\n","        sentence_caps_variations_nl_probs = self.enumerate_caps_variation_nl_probs(adj_counter, sentence_list, n)\n","        # print(collections.Counter(sentence_caps_variations_nl_probs).most_common()[-5:])\n","        return min(sentence_caps_variations_nl_probs, key=lambda sent: sentence_caps_variations_nl_probs[sent])\n"],"execution_count":50,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2t_Rb9TLxKO7"},"source":["# Download the Corpus\n","\n","1. Go [here](https://drive.google.com/open?id=1thAVSW5hzPvNMvvm9RBSkHTHSfTzunfa) to download cs188whw4public.zip. \n","2. Extract the zip locally on your computer.\n","3. Upload the unzipped folder to a folder called \"cs188whw4public\" in your home directory in google drive. (Please double check you have this step correct! If you get file errors, it's likely because your folder name or folder structure is not correct.)\n","\n","Then run the following cell and select the Google account that has the folder with files you just created. \n","\n"]},{"cell_type":"code","metadata":{"id":"1tGX0epKUQkF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606325093979,"user_tz":-330,"elapsed":2172,"user":{"displayName":"Arnav Gulati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXwjHg14lrUBqdz4gJ-8OGJt5S2ZDu5Pp8bMQr=s64","userId":"07163756331818374591"}},"outputId":"b86ed5a1-7604-4ed6-b496-30e8c84e933e"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","path = '/content/drive/My Drive/cs188whw4public/'"],"execution_count":60,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GAs6TB75WHzO"},"source":["# Unit Tests\n","\n","Please run the primitive unit tests below on your code as a sanity check that you implemented everything correctly."]},{"cell_type":"code","metadata":{"id":"lkjwQdmrWJmk","executionInfo":{"status":"ok","timestamp":1606325098233,"user_tz":-330,"elapsed":1815,"user":{"displayName":"Arnav Gulati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXwjHg14lrUBqdz4gJ-8OGJt5S2ZDu5Pp8bMQr=s64","userId":"07163756331818374591"}}},"source":["def test_count_words():\n","    try:\n","        c = N_gram([], 2)\n","        c.text_list = [\"i\", \"need\", \"i\", \"have\"]\n","        actual = c.count_words()\n","        expected = {\"i\": 2, \"need\": 1, \"have\": 1}\n","        return actual == expected\n","    except:\n","        return 0\n","\n","def test_calc_word_probs():\n","    try:\n","        c = N_gram([], 2)\n","        c.text_list = [\"i\", \"need\", \"i\", \"have\"]\n","        actual = c.calc_word_probs()\n","        expected = {\"i\": 0.5, \"need\": 0.25, \"have\": 0.25}\n","        return actual == expected\n","    except:\n","        return 0\n","\n","def test_probs_to_neg_log_probs():\n","    try:\n","        c = N_gram([], 2)\n","        c.text_list = [\"i\", \"need\", \"i\", \"have\"]\n","        actual = c.probs_to_neg_log_probs(c.calc_word_probs())\n","        expected = {\"i\": -np.log(0.5), \"need\": -np.log(0.25), \"have\": -np.log(0.25)}\n","        return actual == expected\n","    except:\n","        return 0\n","\n","def test_filter_adj_counter():\n","    try:\n","        c = N_gram([], 2)\n","        word_tuple = (\"Jimmy\",)\n","        adj_counter = {(\"never\", \"lie\"): 3, (\"hey\", \"Jimmy\"): 2, \n","            (\"Jimmy\", \"Li\"): 1, (\"Jimmy\", \"Carter\"): 100}\n","        actual = c.filter_adj_counter(adj_counter, word_tuple, 2)\n","        expected = {(\"Jimmy\", \"Li\"): 1, (\"Jimmy\", \"Carter\"): 100}\n","        return actual == expected\n","    except:\n","        return 0\n","\n","def test_p_naive():\n","    try:\n","        c = N_gram([], 2)\n","        c.text_list = [\".\", \"i\", \"think\", \".\", \"i\", \"am\", \".\", \"why\", \"yes\", \".\"]\n","        actual = c.p_naive(\"i\", (\".\",), 2)\n","        expected = 2/3\n","        return actual == expected\n","    except:\n","        return 0\n","\n","def test_calc_neg_log_prob_of_sentence():\n","    try:\n","        N = 3\n","        c = N_gram([], N)\n","        c.text_list = [\".\", \"i\", \"think\", \".\", \"i\", \"am\", \".\", \"why\", \"yes\", \".\"]\n","        c.train()\n","        actual = c.calc_neg_log_prob_of_sentence([\"i\", \"am\", \"yes\"], N)\n","        # assumes laplace smoothing, which is already implemented for you.\n","        k = 0.1\n","        expected = sum([-np.log(word_prob) for word_prob in \n","                        [(2 + k) / (3 + 3 * k), (1 + k) / (2 + 3 * k), (k / (1 + 2 * k))]\n","        ])\n","        return np.abs(actual - expected) < 1e-6\n","    except:\n","        return 0\n","\n","def test_calc_prob_of_sentence():\n","    try:\n","        N = 3\n","        c = N_gram([], N)\n","        c.text_list = [\".\", \"i\", \"think\", \".\", \"i\", \"am\", \".\", \"why\", \"yes\", \".\"]\n","        c.train()\n","        actual = c.calc_prob_of_sentence([\"i\", \"am\", \"yes\"], N)\n","        # assumes laplace smoothing, which is already implemented for you.\n","        k = 0.1\n","        expected = np.prod([(2 + k) / (3 + 3 * k), (1 + k) / (2 + 3 * k), (k / (1 + 2 * k))])\n","        return np.abs(actual - expected) < 1e-6\n","    except:\n","        return 0\n"],"execution_count":61,"outputs":[]},{"cell_type":"code","metadata":{"id":"QKpUexcXePkk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606325098234,"user_tz":-330,"elapsed":1297,"user":{"displayName":"Arnav Gulati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXwjHg14lrUBqdz4gJ-8OGJt5S2ZDu5Pp8bMQr=s64","userId":"07163756331818374591"}},"outputId":"8cc823e3-6e23-450d-a6a4-5fd1a532ac12"},"source":["# Run the unit tests\n","q1 = test_count_words()\n","q2 = test_calc_word_probs()\n","q3 = test_probs_to_neg_log_probs()\n","q4 = test_filter_adj_counter()\n","q5 = test_p_naive()\n","q6 = test_calc_neg_log_prob_of_sentence()\n","q7 = test_calc_prob_of_sentence()\n","\n","func_names = [\n","              \"count_words\", \n","              \"calc_word_probs\", \n","              \"probs_to_neg_log_probs\", \n","              \"filter_adj_counter\", \n","              \"p_naive\", \n","              \"calc_neg_log_prob_of_sentence\",\n","              \"calc_prob_of_sentence\"\n","]\n","results = [q1, q2, q3, q4, q5, q6, q7]\n","\n","failed_tests = [func_names[i] for i, result in enumerate(results) if not result]\n","\n","print(\"========PRIMITIVE UNIT TEST RESULTS=========\")\n","if len(failed_tests) == 0:\n","    print(\"YOU GOOD. You passed all the unit tests. You are now ready to generate text!\")\n","else:\n","    print(\"YOU NOT GOOD YET. You have some bugs in functions:\", failed_tests)"],"execution_count":62,"outputs":[{"output_type":"stream","text":["Preprocessing training corpora...\n","Constructing Phrase/Adjacency counters...\n","Constructing List of Phrase Sets...\n","Done Training.\n","Preprocessing training corpora...\n","Constructing Phrase/Adjacency counters...\n","Constructing List of Phrase Sets...\n","Done Training.\n","========PRIMITIVE UNIT TEST RESULTS=========\n","YOU GOOD. You passed all the unit tests. You are now ready to generate text!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GD3q_rv7ySd4"},"source":["# Text Generation\n","\n","Here, we will run code that trains an N-gram model from a given corpus of text files and use the learned conditional probability tables to generate some (mediocre) text.\n","\n","BEFORE RUNNING THE BELOW:\n","make sure you have filled in the required functions from above!\n","1. count_words\n","2. calc_word_probs\n","3. probs_to_neg_log_probs\n","4. filter_adj_counter\n","5. p_naive\n","6. calc_neg_log_prob_of_sentence\n","7. calc_prob_of_sentence"]},{"cell_type":"code","metadata":{"id":"mJja9n7T_gP0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606325517223,"user_tz":-330,"elapsed":1706,"user":{"displayName":"Arnav Gulati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXwjHg14lrUBqdz4gJ-8OGJt5S2ZDu5Pp8bMQr=s64","userId":"07163756331818374591"}},"outputId":"ff0cc61d-9190-47ee-f920-8f8215b828e2"},"source":["training_corpora_path_list = [\n","    \"carter_1980_debate.txt\", \"carter_apr_1977_energy.txt\", \"carter_afghan_invasion.txt\",\n","    \"carter_notre_dame.txt\", \"carter_pres_announce_1974.txt\", \"carter_1979_sotu.txt\", \n","    \"carter_1978_sotu.txt\", \"carter_inaugural.txt\", \"carter_1980_sotu.txt\", \n","    \"carter_concession.txt\", \"carter_crisis_of_confidence.txt\", \"carter_dnc_1976.txt\",\n","    \"carter_dnc_1980.txt\", \"carter_feb_1977_energy.txt\", \"carter_panama_signing.txt\",\n","    \"carter_nov_1977_energy.txt\", \"carter_camp_david.txt\", \"carter_inflation_oct_1978.txt\"\n","]\n","\n","### REDEFINE training_corpora_path_list here if you wish to use your own corpus.\n","\n","### END REDEFINE\n","\n","training_corpora_path_list = [path + file for file in training_corpora_path_list]\n","\n","### START PLAY WITH PARAMS IN CODE HERE ###\n","N = 3\n","k = 5\n","### END PLAY WITH PARAMS IN CODE HERE ###\n","\n","carter_ngram = N_gram(training_corpora_path_list, N)\n","carter_ngram.train()\n","\n","print(\"Generated Sentence:\", carter_ngram.generate_sentence(k))"],"execution_count":68,"outputs":[{"output_type":"stream","text":["Preprocessing training corpora...\n","Training set size: 54178 words\n","Constructing Phrase/Adjacency counters...\n","Constructing List of Phrase Sets...\n","Done Training.\n","Generated Sentence:  but we must face the prospect of failure or mediocrity or to the congress to create programs that we are the kinds of commitments that have been president now for a new department of education .\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zgwLL7gCx5DB"},"source":["# Probabilistic Capitalization\n","\n","In this section, we will run probabilistic arguments to find the most likely capitalization of an inputted sequence of words.\n","\n","The example script below will take very long if you input >8 word sentences. You can speed it up by using the Viterbi Algorithm (not implemented here and not in scope this semester for 188).\n","\n","If you play around, you'll note that this script is not accurate for some slightly out-of-distribution sentences. This can be ameliorated by having a larger training corpus that is ~1-100 million tokens instead of the 50k token corpus we are currently using."]},{"cell_type":"code","metadata":{"id":"Dbj_IU7m_hkQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606325787170,"user_tz":-330,"elapsed":9064,"user":{"displayName":"Arnav Gulati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXwjHg14lrUBqdz4gJ-8OGJt5S2ZDu5Pp8bMQr=s64","userId":"07163756331818374591"}},"outputId":"de28138a-16bb-4b71-f315-d66312ea8823"},"source":["N = 3\n","\n","carter_ngram_cs = CaseSensitive_N_gram(training_corpora_path_list, N)\n","carter_ngram_cs.train()\n","\n","uncapitalized_sentences = [\n","                           \"we are a united people\", \n","                           \"the united states\", \n","                           \"we met soviet challenges in berlin\"\n","]\n","\n","for original_sentence in uncapitalized_sentences:\n","    sentence_list = original_sentence.split(\" \")\n","    print(\"truecased sentence: \", \n","        \" \".join(list(carter_ngram_cs.truecase_sentence(carter_ngram_cs.adj_counters[N], sentence_list, N))))\n","    print(\"original_sentence:\", original_sentence)\n"],"execution_count":69,"outputs":[{"output_type":"stream","text":["Preprocessing training corpora...\n","Training set size: 54177 words\n","Recording all variations of capitalization...\n","Constructing Phrase/Adjacency counters...\n","Constructing List of Phrase Sets...\n","Done Training.\n","[0/8]\n","truecased sentence:  We are a united people\n","original_sentence: we are a united people\n","[0/12]\n","[10/12]\n","truecased sentence:  The United States\n","original_sentence: the united states\n","[0/4]\n","truecased sentence:  We met Soviet challenges in Berlin\n","original_sentence: we met soviet challenges in berlin\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"80LF3mk5QDQ_"},"source":["# Q2: Neural Network Text Generation\n","\n","After completing Question 2 on the homework, "]},{"cell_type":"code","metadata":{"id":"kSI3m2e0Q_YD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606325871579,"user_tz":-330,"elapsed":15329,"user":{"displayName":"Arnav Gulati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXwjHg14lrUBqdz4gJ-8OGJt5S2ZDu5Pp8bMQr=s64","userId":"07163756331818374591"}},"outputId":"a0ddb938-4b21-45f6-acc2-dc5bbbc7bfb0"},"source":["!pip install transformers\n","import argparse\n","import logging\n","\n","import numpy as np\n","import torch\n","import easydict\n","\n","from transformers import (\n","    TransfoXLLMHeadModel,\n","    TransfoXLTokenizer,\n",")"],"execution_count":70,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n","\u001b[K     |████████████████████████████████| 1.3MB 10.8MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 38.0MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Collecting sentencepiece==0.1.91\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 32.4MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Collecting tokenizers==0.9.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 39.6MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=ca194aaea91397cc768f2b98e4c6f42e7cc55fc346ae89713d5d564521f28c0e\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TqntDX1CSSiV","executionInfo":{"status":"ok","timestamp":1606325890208,"user_tz":-330,"elapsed":1436,"user":{"displayName":"Arnav Gulati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXwjHg14lrUBqdz4gJ-8OGJt5S2ZDu5Pp8bMQr=s64","userId":"07163756331818374591"}}},"source":["logging.basicConfig(\n","    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\", datefmt=\"%m/%d/%Y %H:%M:%S\", level=logging.INFO,\n",")\n","logger = logging.getLogger(__name__)\n","\n","MAX_LENGTH = int(100)  # Hardcoded max length to avoid infinite loop\n","\n","MODEL_CLASSES = {\n","    \"transfo-xl\": (TransfoXLLMHeadModel, TransfoXLTokenizer),\n","}\n","\n","\n","def set_seed(args):\n","    np.random.seed(args.seed)\n","    torch.manual_seed(args.seed)\n","    if args.n_gpu > 0:\n","        torch.cuda.manual_seed_all(args.seed)\n","\n","\n","#\n","# Functions to prepare models' input\n","#\n","\n","\n","def prepare_ctrl_input(args, _, tokenizer, prompt_text):\n","    if args.temperature > 0.7:\n","        logger.info(\"CTRL typically works better with lower temperatures (and lower top_k).\")\n","\n","    encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False)\n","    if not any(encoded_prompt[0] == x for x in tokenizer.control_codes.values()):\n","        logger.info(\"WARNING! You are not starting your generation from a control code so you won't get good results\")\n","    return prompt_text\n","\n","\n","def prepare_xlm_input(args, model, tokenizer, prompt_text):\n","    # kwargs = {\"language\": None, \"mask_token_id\": None}\n","\n","    # Set the language\n","    use_lang_emb = hasattr(model.config, \"use_lang_emb\") and model.config.use_lang_emb\n","    if hasattr(model.config, \"lang2id\") and use_lang_emb:\n","        available_languages = model.config.lang2id.keys()\n","        if args.xlm_language in available_languages:\n","            language = args.xlm_language\n","        else:\n","            language = None\n","            while language not in available_languages:\n","                language = input(\"Using XLM. Select language in \" + str(list(available_languages)) + \" >>> \")\n","\n","        model.config.lang_id = model.config.lang2id[language]\n","        # kwargs[\"language\"] = tokenizer.lang2id[language]\n","\n","    # TODO fix mask_token_id setup when configurations will be synchronized between models and tokenizers\n","    # XLM masked-language modeling (MLM) models need masked token\n","    # is_xlm_mlm = \"mlm\" in args.model_name_or_path\n","    # if is_xlm_mlm:\n","    #     kwargs[\"mask_token_id\"] = tokenizer.mask_token_id\n","\n","    return prompt_text\n","\n","\n","def prepare_xlnet_input(args, _, tokenizer, prompt_text):\n","    return prompt_text\n","\n","\n","def prepare_transfoxl_input(args, _, tokenizer, prompt_text):\n","    return prompt_text\n","\n","\n","PREPROCESSING_FUNCTIONS = {\n","    \"transfo-xl\": prepare_transfoxl_input,\n","}\n","\n","\n","def adjust_length_to_model(length, max_sequence_length):\n","    if length < 0 and max_sequence_length > 0:\n","        length = max_sequence_length\n","    elif 0 < max_sequence_length < length:\n","        length = max_sequence_length  # No generation bigger than model size\n","    elif length < 0:\n","        length = MAX_LENGTH  # avoid infinite loop\n","    return length\n","\n","\n","def generate_text(prompt_text=\"\"):\n","    # args = parser.parse_args(argv[1:])\n","    args = easydict.EasyDict({\n","        \"model_type\": \"transfo-xl\",\n","        \"model_name_or_path\" : \"transfo-xl-wt103\",\n","        \"prompt\" : \"\",\n","        \"length\" : 20,\n","        \"stop_token\" : None,\n","        \"temperature\" : 1.0,\n","        \"repetition_penalty\" : 1.0,\n","        \"k\" : 0,\n","        \"p\" : 0.9,\n","        \"padding_text\" : \"\",\n","        \"xlm_language\" : \"\",\n","        \"seed\" : 42,\n","        \"no_cuda\" : False,\n","        \"num_return_sequences\" : 1,\n","        \"device\" : torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n","        \"n_gpu\" : torch.cuda.device_count(),\n","    })\n","\n","    set_seed(args)\n","\n","    # Initialize the model and tokenizer\n","    try:\n","        args.model_type = args.model_type.lower()\n","        model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n","    except KeyError:\n","        raise KeyError(\"the model {} you specified is not supported. You are welcome to add it and open a PR :)\")\n","\n","    tokenizer = tokenizer_class.from_pretrained(args.model_name_or_path)\n","    model = model_class.from_pretrained(args.model_name_or_path)\n","    model.to(args.device)\n","\n","    args.length = adjust_length_to_model(args.length, max_sequence_length=model.config.max_position_embeddings)\n","    logger.info(args)\n","\n","    # Different models need different input formatting and/or extra arguments\n","    requires_preprocessing = args.model_type in PREPROCESSING_FUNCTIONS.keys()\n","    if requires_preprocessing:\n","        prepare_input = PREPROCESSING_FUNCTIONS.get(args.model_type)\n","        preprocessed_prompt_text = prepare_input(args, model, tokenizer, prompt_text)\n","        encoded_prompt = tokenizer.encode(\n","            preprocessed_prompt_text, add_special_tokens=False, return_tensors=\"pt\", add_space_before_punct_symbol=True\n","        )\n","    else:\n","        encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors=\"pt\")\n","    encoded_prompt = encoded_prompt.to(args.device)\n","\n","    output_sequences = model.generate(\n","        input_ids=encoded_prompt,\n","        max_length=args.length + len(encoded_prompt[0]),\n","        temperature=args.temperature,\n","        top_k=args.k,\n","        top_p=args.p,\n","        repetition_penalty=args.repetition_penalty,\n","        do_sample=True,\n","        num_return_sequences=args.num_return_sequences,\n","    )\n","\n","    # Remove the batch dimension when returning multiple sequences\n","    if len(output_sequences.shape) > 2:\n","        output_sequences.squeeze_()\n","\n","    generated_sequences = []\n","\n","    for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n","        print(\"=== GENERATED SEQUENCE {} ===\".format(generated_sequence_idx + 1))\n","        generated_sequence = generated_sequence.tolist()\n","\n","        # Decode text\n","        text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n","\n","        # Remove all text after the stop token\n","        text = text[: text.find(args.stop_token) if args.stop_token else None]\n","\n","        # Add the prompt at the beginning of the sequence. Remove the excess text that was used for pre-processing\n","        total_sequence = (\n","            prompt_text + text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True)) :]\n","        )\n","\n","        generated_sequences.append(total_sequence)\n","        print(total_sequence)\n","\n","    return generated_sequences"],"execution_count":71,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XbsHDcPab-f_"},"source":["Change `prompt_text` to whatever phrase you wish the generated sentence to start with.\n","\n","This script takes really long, ~5-10 mins. It downloads a 1GB Transformer XL model, so every forward pass takes a long time."]},{"cell_type":"code","metadata":{"id":"zG2nAZ_6TWqH","colab":{"base_uri":"https://localhost:8080/","height":541,"referenced_widgets":["442ec9175a0e4175a8058d6f280acb6d","f5cc0b4f8521466e8ac84deea4d894ef","9fe29786139d44f6b41dab61d2952b54","a0f556b355cd4756878d26b47f56f974","1304c4091ea04ebfb23658614c00280f","66235bc593c94ba6b5076127b871ca6a","9219309d063940f2a08b91162fefc74f","c4d3653426ba43038d74ae04ff0f4555","3c705fe2aa0f4edcbd2e768b5b090693","123d63bf5c6148be8ec64a2997ac98ba","bc9f504dc3a345bfa0c9e4fae66b296f","79bc2b1632054ee2a6a3db6991e87682","e0484983966c462f9da0bd18cbe3ae9b","125999ddb9314f71b389d40c253a3879","8587e8826cfd491ca94e7a2b386b79b4","209f8d6905514667b652b1d589508062","a610e1c0f7004f729bb40cc89eab8dac","7549d3d806794503b1d5b03d28dced3f","f31341c6e23b4eeba46fac3d5cbaff31","41e7d666b8b14aa4a94a9ae819721872","fee6fd2a77f149f38c471d19a5767f70","7fc1ca892f9949df9a215d17578e2dae","ead8b69e75a54a26ae85c0d84f5f81e9","2aa3650449d94334a8c1708a5a6ea31e"]},"executionInfo":{"status":"ok","timestamp":1606325992720,"user_tz":-330,"elapsed":66059,"user":{"displayName":"Arnav Gulati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXwjHg14lrUBqdz4gJ-8OGJt5S2ZDu5Pp8bMQr=s64","userId":"07163756331818374591"}},"outputId":"8c45af03-c592-4b66-a454-a814000d3dc5"},"source":["generate_text(prompt_text=\"i played a lot of golf in high school in india\")"],"execution_count":72,"outputs":[{"output_type":"stream","text":["11/25/2020 17:38:47 - INFO - filelock -   Lock 139854982767280 acquired on /root/.cache/torch/transformers/6860d92833eb9d2a42cf185e974ca967fbf4cd58fa8d3d9298e56b9ef7ff8d5c.56c8ef92e693414ef2313bde4ba3679a404de1edbcd5a5780def3971f9706850.lock\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"442ec9175a0e4175a8058d6f280acb6d","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=9143470.0, style=ProgressStyle(descript…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["11/25/2020 17:38:49 - INFO - filelock -   Lock 139854982767280 released on /root/.cache/torch/transformers/6860d92833eb9d2a42cf185e974ca967fbf4cd58fa8d3d9298e56b9ef7ff8d5c.56c8ef92e693414ef2313bde4ba3679a404de1edbcd5a5780def3971f9706850.lock\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["11/25/2020 17:38:49 - INFO - filelock -   Lock 139852853636288 acquired on /root/.cache/torch/transformers/439371439c726aeec36ef037c63b76ae3e384957090780502c2464ca82017ae1.27301fe0016a634426a4129084dcc45038bd1eb454d0b18accf84dcf4adc5563.lock\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3c705fe2aa0f4edcbd2e768b5b090693","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=856.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["11/25/2020 17:38:50 - INFO - filelock -   Lock 139852853636288 released on /root/.cache/torch/transformers/439371439c726aeec36ef037c63b76ae3e384957090780502c2464ca82017ae1.27301fe0016a634426a4129084dcc45038bd1eb454d0b18accf84dcf4adc5563.lock\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/configuration_transfo_xl.py:145: FutureWarning: The config parameter `tie_weight` is deprecated. Please use `tie_word_embeddings` instead.\n","  FutureWarning,\n","11/25/2020 17:38:50 - INFO - filelock -   Lock 139852823710576 acquired on /root/.cache/torch/transformers/891af5f0c8372327a961a768d4ee40b7ca95c428f9384c534e73b9b655c75468.923bd8e0844a782c35f009eddd08a3600739804fbe13bd234f592f36230ab8a9.lock\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a610e1c0f7004f729bb40cc89eab8dac","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1140884800.0, style=ProgressStyle(descr…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["11/25/2020 17:39:22 - INFO - filelock -   Lock 139852823710576 released on /root/.cache/torch/transformers/891af5f0c8372327a961a768d4ee40b7ca95c428f9384c534e73b9b655c75468.923bd8e0844a782c35f009eddd08a3600739804fbe13bd234f592f36230ab8a9.lock\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:434: UserWarning: Setting attributes on ParameterList is not supported.\n","  warnings.warn(\"Setting attributes on ParameterList is not supported.\")\n","11/25/2020 17:39:48 - INFO - __main__ -   {'model_type': 'transfo-xl', 'model_name_or_path': 'transfo-xl-wt103', 'prompt': '', 'length': 20, 'stop_token': None, 'temperature': 1.0, 'repetition_penalty': 1.0, 'k': 0, 'p': 0.9, 'padding_text': '', 'xlm_language': '', 'seed': 42, 'no_cuda': False, 'num_return_sequences': 1, 'device': device(type='cuda'), 'n_gpu': 1}\n","Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n","Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","/usr/local/lib/python3.6/dist-packages/transformers/modeling_transfo_xl.py:445: UserWarning: This overload of nonzero is deprecated:\n","\tnonzero()\n","Consider using one of the following signatures instead:\n","\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n","  indices_i = mask_i.nonzero().squeeze()\n"],"name":"stderr"},{"output_type":"stream","text":["=== GENERATED SEQUENCE 1 ===\n","i played a lot of golf in high school in india pine school and became a subject of a video game in High school. Still, they are subject to\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['i played a lot of golf in high school in india pine school and became a subject of a video game in High school. Still, they are subject to']"]},"metadata":{"tags":[]},"execution_count":72}]},{"cell_type":"markdown","metadata":{"id":"1H7t_4o2HnOp"},"source":["## Fun Resources \n","\n","Yay! You've successfully generated text with a probabilistic N-gram model and a pre-trained Neural Network! \n","\n","Here are some fun links for exploring more of NLP/AI.  \n","\n","*  [Talk To Transformer](https://talktotransformer.com/) - Type stuff and a neural network guesses what comes next\n","* [Thing Translator](https://thing-translator.appspot.com/) - Take pictures of objects and translate them in different languages (use this with your phone)\n","* [CaptionBot](https://www.captionbot.ai/) - Caption photos\n","* [Allen NLP demos](https://demo.allennlp.org/) - They have a lot of different demos you can try out on the left sidebar \n","* [Talk to Books](https://books.google.com/talktobooks/) - Browse books using everyday language\n","* [Tensorflow Playground](http://playground.tensorflow.org) - Play around with a neural network in your browser\n","\n","Take a look at some NLP courses out there. \n","\n"," * Stanford [CS 124](http://web.stanford.edu/class/cs124/) - From Languages to Information\n","\n","* Berkeley [Info 256](http://people.ischool.berkeley.edu/~dbamman/info256.html) - Applied Natural Language Processing\n","\n","* University of Washington [CSE 517](https://courses.cs.washington.edu/courses/cse517/19wi/) - Natural Language Processing\n","\n"]}]}